---
title: "Hypothesis Testing"
subtitle: "p3003 Proteome Bioinformatics Course"
author: "Witek Wolski"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%")
knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo <- FALSE
    options$out.height <- "99%"
    options$fig.width <- 16
    options$fig.height <- 8
  }
  options
})
```



```{css, echo=FALSE}
/* custom.css */
  .left-code {
    color: #777;
      width: 38%;
    height: 92%;
    float: left;
  }
.right-plot {
  width: 60%;
  float: right;
  padding-left: 1%;
}
.plot-callout {
  height: 225px;
  width: 450px;
  bottom: 5%;
  right: 5%;
  position: absolute;
  padding: 0px;
  z-index: 100;
}
.plot-callout img {
  width: 100%;
  border: 4px solid #23373B;
}
```

# Hypothesis testing

- What is hypothesis testing
- What is a p-Value?
- Why p-Values might differ for the same data.
- Modelling distribution of X under null hypothesis becomes very complex very fast for more complex models.
- Often only correct for large sample sizes - asymptotic properties, using CLT results 
- When p-values are usefull?

- __There is an initial research hypothesis of which the truth is unknown.__



---


## Lady tasting tea

Dr. Muriel Bristol, a female colleague of Fisher claimed to be able to tell whether the tea or the milk was added first to a cup.

- The null hypothesis was that the Lady had no such ability.
- The test statistic was a simple count of the number of successes in selecting the 4 cups out of 8.
- What the probability was for her getting the number she got correct?


---

## Lady tasting tea

.pull-left[

```{r teatasting, eval = TRUE}

x <- c(0,1,0,1,1,0,0,1)
x <- combn(x,4)
nrcor <- apply(x, 2, sum)
nulldistr <- table(nrcor)
probs <- nulldistr / sum(nulldistr)
plot(probs)

```
]


.pull-right[


Hence, on $\alpha = 0.05$ reject since getting 4 right is $P(x = 4) = `r probs[5]`$ 

If she would have 3 right would you reject the null hypothesis?
$P(x > 3) = `r probs[5] + probs[4]`$.

]

---

## Long version

- The first step is to state the relevant null and alternative hypotheses. _This is important, as mis-stating the hypotheses will muddy the rest of the process._
- Consider the statistical assumptions being made about the sample in doing the test; _for example, assumptions about the statistical independence or about the form of the distributions of the observations. This is equally important as invalid assumptions will mean that the results of the test are invalid._
- Decide which test is appropriate, and state the relevant test statistic T.
- Derive the distribution of the test statistic under the null hypothesis from the assumptions. _In standard cases this will be a well-known result. For example, the test statistic might follow a Student's t distribution or a normal distribution._
- Select a significance level ($\alpha$), _a probability threshold below which the null hypothesis will be rejected. Common values are $5\%$ and $1\%$._
- The distribution of the test statistic under the null hypothesis partitions the possible values of T into those for which the null hypothesis is rejected—the so-called critical region—and those for which it is not. The probability of the critical region is $\alpha$.
- Compute from the observations the observed value $t_obs$ of the test statistic T.
- Decide to either reject the null hypothesis in favor of the alternative or not reject it. _The decision rule is to reject the null hypothesis $H_0$ if the observed value $t_obs$ is in the critical region, and to accept or "fail to reject" the hypothesis otherwise._

---

## Brief version

- Distribution of the T statistics under null hypothesis
- with critical region for $\alpha = 0.05$.
- Plot a $T_{obs}$ within the critical region and outside the critical region.

---


.pull-left[
```{r hypo-explained, eval=FALSE}
# Plot distribution of T under null Hypothesis
x <- seq(-3,5, by=0.1)
plot(x, dnorm(x), type="l", xlim = c(-3,5))

# Specify alpha and show critical regions
alpha <- 0.05
crl <- qnorm(alpha/2)
crh <- qnorm(1-alpha/2)
abline(h = 0)
abline(v = c(crl, crh), col=2)
text(2.1,0.1, "critical region", adj=0, srt=90)
text(-2.1,0.2, "critical region", adj=0, srt=-90)

# Show not significant test statistic
abline(v = 1.2, col=3)
text(1.2, 0.1, expression(t[obs]), srt=90)

# Show significant test statistic
abline(v = 4, col="magenta")
text(4, 0.1, expression(t[obs]), srt=90)

```
]


.pull-right[
```{r hypo-explained-out, ref.label="hypo-explained", echo=FALSE}
```
]

---

## Abbreviated version

- __State research hypothesis__
- State Relevant Null and Alternative hypothesis
- Specify test statistic 
- What is the distribution of the test statistics when null hypothesis is true? - difficult
- What is $t_{obs}$? 

---

## Testing if mean is equal to $\mu$

- research hypothesis - mean of sample is different than some value $\mu$.

- What is the null and what is the alternative?
- Null is that the mean of observed data is equal to $\mu$.
- That it is NOT equal to $\mu$.

- What is the distribution of the observations?
- Our observations are independent, identically distributed (iid)
- Normal. $x \sim N(\mu, \sigma)$.

- State the relevant test statistic T.
- A suitable test statistics might be $\bar{X} - \mu$.
- But we do need to know the distribution of the test statics under null.
- Clearly it will depend on samples size $n$ and on the variance $\sigma^2$

- A test statistics we know the distribution of is: 
$$T = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$$

---

## Simulating some data from H0


.pull-left[


```{r, include=FALSE}
getBreaks <- function(T0,by=0.1){
  res <- seq(round(min(T0), digits = 1)-by,round(max(T0), digits = 1)+by, by=by)
  return(res)
}
```

```{r simulateData, eval=FALSE}
# Simulating data from Null
N <- 10000
N_obs <- 4
mu <- 0
sigma <- 1

bb <- function(y){
  x <- rnorm( N_obs, mu, sigma )
  data.frame(mean = mean( x ), sd = sd(x)) 
}
res <- purrr::map_df(1:N, bb)
par(mfrow=c(2,1))
hist(res$mean, main="distribution of mean")
hist(res$sd, main="distribution of sd")
```
]


.pull-right[
```{r simulateData-out, ref.label="simulateData", echo=FALSE}
```
]


---

## Testing if mean is equal to $\mu$ with KNOWN variance

.pull-left[
if $\sigma$ known

```{r simulatingH0SigmaK, eval=FALSE}
T0 <- res$mean/(sigma/sqrt(N_obs))
hist(T0, breaks=getBreaks(T0), xlim=c(-6,6),
     probability = T, ylim=c(0,0.4), main="")
x <- seq(-10,10,0.1)
lines(x, dnorm(x),col=2)

```

$T|H_0 \sim N(0,1)$

]

.pull-right[
```{r simulatingH0SigmaK-out, ref.label="simulatingH0SigmaK", echo=FALSE}
```
]

---

## Testing if mean is equal to $\mu$ with UNKNOWN variance

.pull-left[

if $\sigma$ UNKNOWN

```{r simulatingH0SigmaUK, eval = FALSE}
T0 <- res$mean/(res$sd/sqrt(N_obs))
hist(T0,
     breaks=getBreaks(T0),
     probability = T, xlim=c(-6,6),ylim=c(0,0.4), main="")

x <- seq(-10,10,0.1)
lines(x, dnorm(x),col="red")
lines(x,dt(x,df = N_obs),type="l",col="green",lwd=2)

```

$T|H_0 \sim T(\mu=0,df=N_{obs})$
]


.pull-right[
```{r simulatingH0SigmaUK-out, ref.label="simulatingH0SigmaUK", echo=FALSE}
```
]

---

## Testing if mean is equal to $\mu$ with UNKNOWN variance

.pull-left[

```{r simulatingH0withWrongAssumptions, eval=FALSE}
# Simulating data from Null
N <- 10000
N_obs <- 4
rate <- 1
bb_exp <- function(y){
  x <- rexp( N_obs, rate=rate )-1
  data.frame(mean = mean( x ), sd = sd(x)) 
}

res <- purrr::map_df(1:N, bb_exp)
T0 <- res$mean/(res$sd/sqrt(N_obs))

hist(T0,
     breaks=getBreaks(T0),
     probability = T, xlim=c(-6,6),ylim=c(0,0.4), main="")
lines(x,dt(x,df = N_obs),type="l",col="green",lwd=2)
```
]


.pull-right[
```{r simulatingH0withWrongAssumptions-out, ref.label="simulatingH0withWrongAssumptions", echo=FALSE}
```
]

---

## Testing if mean is equal to $\mu$ with UNKNOWN variance

.pull-left[

```{r simulatingH0withWrongAssumptionsN12, eval=FALSE}
# Simulating data from Null
N <- 10000
N_obs <- 40
rate <- 1
bb_exp <- function(y){
  x <- rexp( N_obs, rate=rate )-1
  data.frame(mean = mean( x ), sd = sd(x)) 
}

res <- purrr::map_df(1:N, bb_exp)
T0 <- res$mean/(res$sd/sqrt(N_obs))

hist(T0,
     breaks=getBreaks(T0),
     probability = T, xlim=c(-6,6),ylim=c(0,0.4), main="")
lines(x,dt(x,df = N_obs),type="l",col="green",lwd=2)
```
]


.pull-right[
```{r simulatingH0withWrongAssumptionsN12-out, ref.label="simulatingH0withWrongAssumptionsN12", echo=FALSE}
```
]

---

# Central Limit Theorem

- In probability theory, the __central limit theorem__ (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. 

- The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.


---

# Two sample t-test

$T = \frac{Y_1 - Y_2}{\sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}}$

Significance level $\alpha$

Reject the null hypothesis that the two means are equal if

$|T| > t_{1-\alpha/2,v}$

with $v$ degrees of freedom 


$\upsilon = \frac{(s^{2}_{1}/N_{1} + s^{2}_{2}/N_{2})^{2}} {(s^{2}_{1}/N_{1})^{2}/(N_{1}-1) + (s^{2}_{2}/N_{2})^{2}/(N_{2}-1) }$

---

# Randomization tests

But what if we want to analyse the data without assuming
normality? E.g., because the sample sizes are small.

---

## Algorithm

1. Suppose the 16 individuals in the study have been labelled


```{r echo=FALSE}
tmp <- list("Diet A"=as.integer(c( 1, 2, 3, 4, 5)),
            "Diet B" = as.integer(c( 6, 7, 8 , 9, 10)))

library(flextable)
tmp <- (data.frame(tmp))

tmp %>%
  head() %>%
  t() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  flextable()

```

2. Randomly re-assign the 16 individuals to the two groups.
3. Re-calculate the test-statistic for this permuted data
4. Repeat 2 and 3 to obtain $B$ sampled test-statistics, denoted
$T_1, \dots, T_B$.
5. For a two-sided test, the estimated p-value of the observed
test statistic $T_{obs}$ is:

$\frac{1}{B} \sum^B_{i=0} I_{T_i} >= |T_{obs}|$


---

## Example A


.pull-left[

```{r randomization_test, eval=FALSE, echo=FALSE}

rm(list=ls())
library(coin)


dist1 <- function(N){rnorm(N, 0, 1 )}
dist2 <- function(N){rnorm(N, 1.3,1.5)}

sample_stats <- function(N = 12, dist1, dist2, samples = 100){
  
  xx <- combn(N, N/2)
  
  res_p <- matrix(nrow=samples, ncol=4)
  colnames(res_p) <- c("our","coin","t.test","asymp.test")
  means <- NULL
  h_0_norm <- NULL
  t_obs <- NULL
  for(i in 1:samples){
    sample <- data.frame(intensity = c(dist1(N/2), dist2(N/2)),
                         treatment = c(rep("A", N/2), rep("B", N/2)))
    
    teststat <- function(i, comb, data){res <- mean(data[comb[,i],1]) - mean(data[-comb[,i],1]); return(res)}
    h_0_norm <- sapply(1:ncol(xx), teststat, xx, sample)
    
    means <- aggregate(intensity ~treatment, sample, mean)
    t_obs <- means[2,2] - means[1,2]
    
    our <- sum(abs(t_obs) <= abs(h_0_norm)) / length(h_0_norm)
    coin <- pvalue(independence_test(intensity ~treatment, data = sample ,distribution = exact()))
    t.test <- t.test(intensity~ treatment, data = sample)$p.value
    asymp.test <- asympTest::asymp.test(intensity~ treatment, data = sample)$p.value

    res_p[i,]<-c(our, coin, t.test, asymp.test)
  }
  return(list(res_p = res_p, sample=sample , means=means, h_0_norm = h_0_norm, t_obs =t_obs))
}

xx <- sample_stats(N=10, dist1, dist2)
par(mfrow=c(1,4))
boxplot(intensity ~treatment, data = xx$sample)
points(xx$means[,2],col="red",pch="-", cex=4)
breaks <- seq(-5,5,by=0.1)
hist(xx$h_0_norm, breaks=breaks, freq = F, xlim=c(-3,3), ylim=c(0,1), density=8,angle=-45)
abline(v = xx$t_obs, col="green",lwd=2)


plot(xx$res_p[,"coin"], xx$res_p[,"t.test"], log="xy", ylab="t.test", xlab="randomization test")
points(xx$res_p[nrow(xx$res_p),"coin"], xx$res_p[nrow(xx$res_p),"t.test"],col=2, pch="x", cex=2)
abline(c(0,1), col="blue")

plot(xx$res_p[,"asymp.test"], xx$res_p[,"t.test"], log="xy",  xlab="asymptotic test",ylab="t.test")
points(xx$res_p[nrow(xx$res_p),"asymp.test"], xx$res_p[nrow(xx$res_p),"t.test"],col=2, pch="x", cex=2)
abline(c(0,1), col="blue")



```



]

.pull-right[
```{r randomization_test-out, ref.label="randomization_test", echo=FALSE}
```
]

## Example B

----


.pull-left[
```{r randomization_test_ex2}
dist1 <- function(N){rnorm(N, 1, 1 )}
dist2 <- function(N){rexp(N, 0.4)*2}

xx <- sample_stats(N=10, dist1, dist2)
par(mfrow=c(1,4))
boxplot(intensity ~treatment, data = xx$sample)
points(xx$means[,2],col="red",pch="-", cex=4)
breaks <- seq(-10,10,by=0.1)
hist(xx$h_0_norm, breaks=breaks, freq = F, xlim=c(-3,3), ylim=c(0,1), density=8,angle=-45)
abline(v = xx$t_obs, col="green",lwd=2)


plot(xx$res_p[,"coin"], xx$res_p[,"t.test"], log="xy", ylab="t.test", xlab="randomization test")
points(xx$res_p[nrow(xx$res_p),"coin"], xx$res_p[nrow(xx$res_p),"t.test"],col=2, pch="x", cex=2)
abline(c(0,1), col="blue")
abline(v=0.05, h=.05, col="red")

plot(xx$res_p[,"asymp.test"], xx$res_p[,"t.test"], log="xy",  xlab="asymptotic test",ylab="t.test")
points(xx$res_p[nrow(xx$res_p),"asymp.test"], xx$res_p[nrow(xx$res_p),"t.test"],col=2, pch="x", cex=2)
abline(c(0,1), col="blue")
abline(v=0.05, h=.05, col="red")

```
]


.pull-right[
```{r randomization_test_ex2-out, ref.label="randomization_test_ex2", echo=FALSE}
```
]


---

# Take home message


- __Think about your resarch hypothesis of which the truth is unknown.__
- What is H_0?


- Very difficcult to get it right distribution under H_0.
- Usually asymptotic properties are used (e.g. CLT)
- Impossible to get it right for the sample sizes we have.
- Some software for high throughput proteomics therefore reports other types of probabilities.
