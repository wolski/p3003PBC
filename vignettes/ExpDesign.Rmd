---
title: "Experimental Design"
subtitle: "FGCZ Protein Informatics Training"
author: "Witold Wolski"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "metropolis", "metropolis-fonts", "trug-ggplot2.css"]
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
editor_options:
  chunk_output_type: console
vignette: >
  %\VignetteIndexEntry{Experimental Design} 
  %\VignetteEncoding{UTF-8}
  
  %\VignetteEngine{knitr::rmarkdown}
---

class: fullscreen, inverse, top, center, text-black
background-image: url("../inst/images/Linear_chair.jpg")

.font150[**design**]

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 4.25,
                      fig.height = 3.5,
                      fig.retina = 3,
                      message = FALSE,
                      warning = FALSE,
                      cache = TRUE,
                      autodep = TRUE,
                      hiline = TRUE)

knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo <- FALSE
    options$out.height <- "99%"
    options$fig.width <- 16
    options$fig.height <- 8
  }
  options
})
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  if (!is.null(options$hiline) && options$hiline) {
    x <- stringr::str_replace(x, "^ ?(.+)\\s?#<<", "*\\1")
  }
  hook_source(x, options)
})
options(htmltools.dir.version = FALSE, width = 90)
as_table <- function(...) knitr::kable(..., format='html', digits = 3)
```


---

exclude: true

# LFQ a success story


--
exclude: true

- Novel aquisition methods (DIA vs DDA)

.img-right[
![](../inst/images/DIA_vs_LFQ.jpg)
]

--
exclude: true

- improvements in feature detection and extraction

.img-right[
![](../inst/images/IonStar_MQ_PD.jpg)
]

--
exclude: true

- a zoo of data <br/> transformation, pretreatment, imputation methods

.img-right[
![](../inst/images/LFQ_Improvement.jpg)
]


--
exclude: true

- improvements in fold change estimation

.img-right[
![](../inst/images/Ropeca.png)
]



.footnote[
[Extending the Limits of Quantitative Proteome](https://www.mcponline.org/content/14/5/1400);
[Ionstar](https://www.pnas.org/content/115/21/E4767);
[Simultaneous Improvement in the Precision...](https://www.mcponline.org/content/18/8/1683);
[Ropeca](https://www.nature.com/articles/s41598-017-05949-y)
]

---

# Overview

- Question of Causality
- Judgement under uncertainty
- Scientific method
- Causality and Bradford Hill criteria
- Study Types
- Confounders
- Exp. Design
- Sample Size

---

# Question of Causality

.pull-left[
Assess whether a particular __agent__ 
caused or influenced a particular __outcome__ 

- What is the probability that A belongs to class B?
- What is probability that A originates from B?
- What is probability that B generates A?
]

.right-plot[
![](../inst/images/CausalNetworkSignalTransduction.png)
]


---

# Judgement under uncertainty - Heuristics and biases

In answering these question people typically rely on;

- __representativeness__ heuristic,
  - i.e. when A is highly representative of B, the probability that A originates from B is judged to be high.
   (Bat and Bird , Frog and Tadpole)
- __availability__ heuristic
  - retrievability - how easily information can be brought to mind
  - imaginability - how easy is it to explain outcomes (a good story sells)
  - bias due to illusory correlations - frequency of co-occurrence
- __anchoring__ heuristic
  - insufficient adjustment
  - assessment of uncertainty - typically under-estimated


---

# Judgement under uncertainty - Heuristics and biases


and people are insensitive to:
- to prior probability of outcome
- to sample size
- to predictability (predict future based on observations made now).
- illusion of validity caused by consistency  $^{\star}$
- misconception of regression - confounding $^{\star\star}$


.footnote[
$^{\star}$ People tend to have great confidence in predictions based on redundant input variables. However, consistent patterns are most often observed when input variables are highly redundant or correlated. <br/>

$^{\star\star}$ Example : Because the instructors had praised their trainees after good landings and admonished them after poor
 ones, they reached the ... conclusion that punishment is more effective than reward.

[Judgment under Uncertainty: Heuristics and Biases by Tversky A. and Kahneman D. Science 1974](https://www.science.org/doi/10.1126/science.185.4157.1124),

]



---

exclude: true
# Judgement under uncertainty - Heuristics and biases

Imagine an urn filled with balls, of which 2/3 are of one color and 1/3 of another.

- One individual (__A__) has drawn $5$ balls from the urn, and found that $4$ were red and $1$ was white. 
- Another (__B__) has drawn $20$ balls and found that $12$ were red and $8$ were white. 

Which of the two individuals should feel more confident that the urn contains $2/3$ red balls and $1/3$ white balls, rather than the opposite? 

What odds should each individual give?

- Odds for __A__ (5 balls : 4 red; 1 white) =
- Odss for __B__ (20 balls : 12 red; 1 white) =  

.footnote[Odds : $$\frac{\textrm{Probability of urn with 2/3 RED balls}}{\textrm{Probability or urn with 2/3 WHITE balls}}$$ ]


---
# Scientific Method

1. __Question__
2. __Hypothesis__
3. __Experiment__ <br/> ordered investigation that attempts to prove or disprove a hypothesis
  - must show if hypothesis is supported or not.
  - results must be measurable
  - experiment must be repeatable
4. __Observation__ <br/> make observations about results of experiment
5. __Analysis__ <br/> run NHST (Frequentist) or derive posterior distribution (Bayesian) $^{\star}$
6. __Conclusion__ <br/> significant result

.footnote[NHST - null hypotheses significance test <br/>
$^{\star}$ or barplot with error bars, or image of a blot.]

---
exclude: true

# Causality - How to prove cause and effect?

.right-plot[
![](../inst/images/BuehlmanCausality.png)
]

.footnote[[Peter Buehlmann "Invariance, Causality and Robustness"](https://arxiv.org/pdf/1812.08233.pdf)]

---

# Causality - Bradford-Hill Criteria


1.  __Strength of association__ – (effect size) the greater the effect compared with those not
exposed to the agent the more plausible is the association
2. __Consistency__ – (reproducibility) does it happen in other groups of people – both men and
women, different countries
3. __Specificity__ – no other likely explanations
4. __Temporality__ – effect follows cause, and if expected delay, effect must occur after that delay
5.  __Biological gradient__ – the stronger the agent the greater the effect
6.  __Plausibility__ – is there a possible biological mechanism that could explain the effect
7. __Coherence__ – do different types of study result in similar conclusions – e.g., controlled trials and observational studies
8. __Experiment__ "Occasionally it is possible to appeal to experimental evidence"
9. __Analogy__  similar agents have similar results... asbestos and carbon nanotubes

.footnote[

(Austin Bradford Hill, Proc R Soc Med. 1965 May; 58(5): 295–300.)[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1898525/]

]
---


# Causality - Bradford-Hill Criteria

- Protein Quantification 
  - Strength of association : Control, Treatment $5 \mu g$, Treatment $10 \mu g$
  - Consistency : Same effect in different genetic backgrounds, different laboratories.
  - Temporality : Time course data
  - Specificity : A treatment has only a single effect (e.g. only the expression of one protein changes)

- Which Bradford-Hill criteria do you aim to meet in your study?

[Applying the Bradford Hill criteria in the 21st century](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4589117/)

.footnote[
Sir Austin Bradford Hill CBE FRS was an English epidemiologist and statistician, pioneered the randomized clinical trial (use of streptomycin in treating tuberculosis) and, together with Richard Doll, demonstrated the connection between cigarette smoking and lung cancer (case-control study).
]

---

# Causality - Bradford-Hill Criteria

.pull-left[Plausibility<br/>
![](../inst/images/BH_Plausibility.png)]

.pull-right[Specificity<br/>
![](../inst/images/BH_Specificity.png)]

.footnote[[Explaining Hill Criteria using xkcd](https://www.kdnuggets.com/2017/02/hill-data-scientist-xkcd-story.html)<br/>
[The Bradford Hill Criteria Don't Hold Up](https://lesslikely.com/statistics/bradford-hill-criteria-dont-hold/)
]


---

exclude: true

# Causality

.pull-left[
- Can we infer causal models directly from data?
- Would you prefer?
  - to have dataset with $4000$ proteins and $2$ conditions
  - a dataset with $400$ proteins and more than $>100$ conditions?
]

.right-plot[
![](../inst/images/Causality_fromData.jpg)

]
.footnote[[CausalDisco](https://github.com/annennenne/causalDisco/blob/master/slides/causaldisco_ahp_user2019.pdf)
an overview of methods (implemented in R) to infer causal networks from data <br/>

Judea Pearl "The Book of Why: The New Science of Cause and Effect"<br/>

]


---

# Study Types - 530 B.C.

“Test your servants for ten days:<br/> Give us nothing but vegetables to eat and water to drink.<br/> Then compare our appearance with that<br/> of the young men who eat the royal food, <br/>and decide what to do with us based on how we look.”

Bible (see Daniel 1:1–16).<br/>


.img-right[
![](../inst/images2021/Bergmann_Murdoch.png)
]

.footnote[Bregman, Rutger. Utopia for Realists (p. 206). Bloomsbury Publishing. Kindle Edition. ]



---

# Study Types 

.pull-left[
## Observational study
- Case-control study -  individuals with a specific characteristic (disease and similar individuals without disease)
- Cross-sectional study - aim to provide data on the entire population under study.
- Longitudinal study - repeated observations of the same variables (e.g., people) over short or long periods of time. They can also be structured as longitudinal __randomized experiments__, e.g. Mice aging.
]

.pull-right[
## Experimental Intervention
- __randomized controlled trials__  <br/>
(randomize subjects into two groups, placebo and treatment group)
]


---

# Study Types - Case control study
.image-70[
![](../inst/images/Study_Type_Case_Control.png)
]


---

exclude: true

# Study Types - Randomized controlled trial (RCT)

"This year’s (2019) Laureates have introduced<br/>
a new approach to obtaining reliable answers<br/>
about the best ways to fight global poverty. 

In brief, it involves __dividing this issue into<br/>
smaller, more manageable, questions__ – for example,<br/>
the most effective interventions for improving educational<br/>
outcomes or child health. 

They have shown that these smaller, more precise,<br/>
questions are often best answered via carefully<br/>
__designed experiments__<br/>
among the people who are most affected."


.img-right[
![](../inst/images/Nobel2019.png)
]




---
layout: false

# Study Types - RCT : __Randomization__

A randomized controlled trial is a type of scientific experiment that aims to reduce certain sources of __bias__ when testing the effectiveness of new treatments.

- __selection bias__ 
    - sample obtained is _not representative_ of the population intended to be analyzed.
- __allocation bias__ 
    - _systematic difference_ in how participants are assigned to treatment groups and comparison groups
    - __Randomly allocate subjects to two or more groups__


.footnote[

https://en.wikipedia.org/wiki/Randomized_controlled_trial<br/>
https://en.wikipedia.org/wiki/Bias]

---

# Study Types -  __Blinding__

 - Information which may influence the participants<br/>
 is withheld until after the experiment is completed
 - A blind can be imposed on any participant<br/>
 of an experiment, including<br/> 
 __subjects, researchers, technicians,<br> data analysts, and evaluators__

- In clinical Trials double blind trials should <br/>
be used where possible
  - __Single blind__ – either patient or evaluator blind.
  - __Double blind__ – both patient and evaluator blind

- To prevent __ascertainment__ and __performance__ __bias__.

.img-right[![](../inst/images/blind_trials.png)]

.footnote[[Blinding: Who, what, when, why, how?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947122/)]



---


# Study Types

.pull-left[
## Observational study
pros:
- May require less resources
- Less ethical considerations (e.g. smokers)
- Good if outcome of interest is rare

cons:
- difficult to determine causality
- no randomization or blinding
- The exposure status is not determined by the researcher
]

.pull-right[
## Experimental Intervention
pros:
- More validity
- Can determine causality
- Randomized and blinded


cons:
- may require more resources
- Ethical concerns for certain exposures
- Difficult if outcome studied is rare<br/> (e.g. Vaccination)
]

---



# Confounders


Correlation does not imply causation but causation may imply correlation.

__Confounder__ - is a variable that influences both the dependent variable and independent variable, causing a spurious association.

__Reichenbach's common cause principle:__ <br/>
A correlation occurs due to one of the three possible mechanisms
.img-small[
![](../inst/images/Causality_Mechanisms_3.png)
]

.footnote[[Mediators, confounders, colliders – a crash course in causal inference](https://theoreticalecology.wordpress.com/2019/04/14/mediators-confounders-colliders-a-crash-course-in-causal-inference/)]

<!--
.img-right[
![](../inst/images/Confounding_Wiki.png)
]
-->

---

# Confounders - Example

- Flight Training Experienced instructors noted: 
  - that praise for an exceptionally smooth landing is typically followed by a poorer landing on the next try.
  - harsh criticism after a rough landing is usually followed by an improvement on the next try.
The instructors concluded that verbal rewards are detrimental to learning while punishments are beneficial, contrary to accepted psychological doctrine.

.footnote[[Judgment under Uncertainty: Heuristics and Biases by Tversky A. and Kahneman D. Science 1974](https://www.science.org/doi/10.1126/science.185.4157.1124)]

--
Correct explanation : regression to the mean.

---
layout: false

# Confounders - Controlling for 

- Randomize over biological and <br/>  technical co-variates. <br/> 
  e.g. run Id, age.
- Avoid batch effects:
    - process all samples in parallel
- block confounders <br/> 
_complete block design_, i.e.,<br/> 
all treatments are present in each batch<br/>
in equal numbers.

.img-right[
![How to avoid](../inst/images/BatchEffect.png)
How to avoid confounding by batch
]


.footnote[
[Statistical Design of Quantitative Mass Spectrometry-Based Proteomic Experiments](https://doi.org/10.1021/pr8010099)<br/>
[Importance of Block Randomization When Designing Proteomics Experiments](https://doi.org/10.1021/acs.jproteome.0c00536)
]

---

# Confounders - Controlling for 

## Queue Generator

Use block random queue when processing samples to _block_ for changes in the chromatographic column over time.

__Example:__

\# of samples in each condition 3 x A, 3 x B, 2 x C, 2 x D. <br/>
Blocks :  ABCD, BCDA, BA <br/>


.footnote[http://fgcz-ms-shiny.uzh.ch:8080/queue_generator10/]

---

# Confounders - Reporting

Document possible co-variates, e.g.,

.pull-left[

- Human subjects:
  - age
  - bmi 
  - gender 
  - ...
]

.pull-right[
-  Biochemical/Technical: 
   - batch 
   - run Id
   - instrument
   - ...
]

- Report your research so that it can be reviewed, reproduced and analysed.


.footnote[
[Over-interpretation and misreporting of prognostic factor
studies in oncology: a systematic review](https://www.nature.com/articles/s41416-018-0305-5.pdf)<br/>
http://www.equator-network.org/<br/>
https://www.ncbi.nlm.nih.gov/pubmed/29873743
]



---


# Exp. Design

The key technical issue is whether comparisons are made __between__ or __within__ subjects.


.pull-left[
Between

- Parallel Group Design <br/> k - groups, $n_i$ patients in group $i$ receive treatment $i$
- Factorial Design <br/>  - more than one factor, e.g. treatments and knockout.
]

.pull-right[
Within (repeated/paired)

- In series design <br/> each patient all $k$ treatments in same order
- crossover design <br/> each patient  all $k$ treatments in different order
]

Are combinations of both possible?

.footnote[Ronald Fisher (1890 - 1962) "The Arrangement of Field Experiments" (1926) and "The Design of Experiments" (1935).]

---

layout: false

# Exp. Design - Interactions

.footnote[[Understanding Interaction Effects in Statistics](https://statisticsbyjim.com/regression/interaction-effects/)]


Q : "Do you prefer ketchup or chocolate <br/> sauce on your food?"

--

A : "__It depends__ on the type of food!" <br/>

.img-right[
![](../inst/images/HotdogKetchup.jpg)
]

--

Think of interaction effects as an __"it depends"__ effect. <br/>

You cannot answer the question without __more__ information <br/>
about the other variable in the interaction term.<br/>

Factorial Designs are efficient at evaluating 
the effects and possible interactions of
several factors (independent variables).


---

# Exp. Design - Factorial vs parallel group

.pull-left[
Parallel (one factor)

- drug A
- drug B
- plac
]

.pull-right[
Factorial (2 or more factors)

- Factor A
  - drug A
  - plac A
- Factor B
  - drug B
  - plac B
]

40 patients, Placebo, drug A and drug B.
How would you allocate these patients?


.footnote[Ronald Fisher pioneered the use of factorial experiments instead of the one-factor-at-a-time method.]

---

# Exp. Design - Factorial vs parallel group

.pull-left[
Parallel (one factor)

- drug A (13)
- drug B (13)
- plac (14)

Compare:
- 13 drug A vs 14 plac
- 13 drug B vs 14 plac
]

.pull-right[
Factorial (2 or more factors)

- drug A + drug B (10)
- plac A + drug B (10)
- drug A + plac B (10)
- plac A + plac B (10)

Compare:
- 20 drug A vs 20 plac A
- 20 drug B vs 20 plac B 
]


---
layout: false

# Exp. Design - Interactions

Factorial Designs are efficient at evaluating<br/>
the effects and possible interactions of<br/> 
several factors (independent variables).


.footnote[https://en.wikipedia.org/wiki/Design_of_experiments]

--

- No interaction
.img-right[
![](../inst/images/TypesOfInteractions_NO.png)


[Medical Statistics - University of Sheffield]
]


--

- Quantitative interaction

.img-right[
![](../inst/images/TypesOfInteractions_Quanti.png)
]


--

- Qualitative interaction

.img-right[
![](../inst/images/TypesOfInteractions_Quali.png)
]



---
exclude: true

# All possible interactions


- 1 = there was a main effect for IV1.
- ~1 = there was not a main effect for IV1
- 2 = there was a main effect for IV2
- ~2 = there was not a main effect of IV2
- 1x2 = there was an interaction 
- ~1x2 = there was not an interaction

.img-right[
![](../inst/images/allInteractions2x2.png)
]

.footnote[https://crumplab.github.io/statistics/more-on-factorial-designs.html]

---


# Sample Size - Ethical considerations

- It is agreed that it is __unethical__ _to conduct research which is badly planned or
executed_.

-  It is unethical to perform a trial which:
  - has _many more subjects_ than are needed to reach a conclusion.
  - has little prospect of reaching any conclusion, <br/> e.g. because 
  of _insufficient numbers of subjects_ (or some other aspect of poor design)

- The local ethics committee has discretion on how it will supervise __non-interventional__ studies
  -  US - Institutional Review Board (IRB)
  -  EU - ethics committees

[Declaration of Helsinki (1964+amendments))](https://www.wma.net/policies-post/wma-declaration-of-helsinki-ethical-principles-for-medical-research-involving-human-subjects/)

---

# Sample Size - Types of error


A __type I error__ (false positive) occurs when<br/> the null hypothesis (H0) is true, but is rejected.<br/> 
The _type I error rate_ or __significance level__ (p-Value)<br/> is the probability of rejecting the<br/> 
null hypothesis given that it is true.


A __type II error__ (false negative) occurs when<br/> 
the null hypothesis is false,<br/>
but erroneously fails to be rejected. <br/>
The _the type II error rate_ is denoted by the Greek letter $\beta$<br/>
and is related to the __power of a test__ (which equals $1−\beta$).

For a given test, the only way to reduce both error rates<br/> 
is to __increase the sample size__, and this may not be feasible.

.img-right[
![](../inst/images/hl_nullhypo_errors.png)
]


---

# Sample Size - Sample size calculation


.pull-left[

Run pilot experiment and measure the coefficient of variation (CV) or <br>
$\sigma^2$ of replicates:

- technical
- _biochemical_
- __biological__
]



.pull-right[
- _biological variance >> bio-chem+tech variance_<br/> 
  - Only provide biological replicates
- _bio-chem+tech variance >> biological variance_<br/>
  - __improve sample handling and preparation__
  - choose different technology
  - buy better instrument
]

.footnote[measure 3 tech replicates, 3 tech+bio-chem replicates, 3 tech+bio-chem+biological replicates]

---

# Sources of variation

.footnote[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6807909/]


.left-code[
- (a) the instrumental variation 
- (b) the total technical variation 
- (c) the intrabiological variation
- (d) the interbiological variation 
]

.img-right[
![](../inst/images/VariousTypesOfVariation.jpg)
![](../inst/images/VariousTypesOfVariation_Summary.jpg)
]


---
class: my-one-page-font

# Two sample t-test

Test statistic $$T = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$$
Null distribution $$T|H_{null} \sim T(0,1,df = N-1)$$

where delta = $\bar{X}_2 - \bar{X}_1$, sd - standard deviation, df - depends on sample size


---
# `power.t.test` - estimating power

Compute the power of the one- or two- sample t-test, or determine parameters to obtain a target power.

```{r, eval = FALSE}
power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
             power = NULL,
             type = c("two.sample", "one.sample", "paired"),
             alternative = c("two.sided", "one.sided"),
             strict = FALSE, tol = .Machine$double.eps^0.25)
```

Relates the type-2 error (power) to the other variables: type-1 error (sig.level), standard deviation (sd), effect size (delta), sample size (n)

---

# `power.t.test` - estimating power


What is __the power__ of the two-sample t-test using 
a significance level of $0.05$ if we want to detect a biologically relevant difference of $50\%$
given a standard deviation of $0.5$ and group size of $10$ samples?


.remark-code[
```{r}
power.t.test(delta = 0.59, n = 10, sd = 0.5, sig.level = 0.05)
```
]

.footnote[ Why `delta = 0.59`? $50%$ difference, means FC of $1.5$ or greater, and $log2(1.5) = 0.59$]


---
layout: false

# `power.t.test` - esitmating difference $d$


What __biologically relevant difference__ (or greater)
 can be detected with a two-sample t-test using 
a significance level of $0.05$ and a power of $0.8$ 
given a standard deviation of $0.5$ and group size of $10$?

.remark-code[
```{r}
res <- power.t.test(n = 10, sd = 0.5, power = 0.8, sig.level = 0.05)
res$delta
2^res$delta
```
]

$$
\begin{aligned}
log_2(x) = y&~~~\textrm{exactly if} ~~~ 2^y = x\\
log_2(I_1/I_2) &= log2(I_1) - log2(I_2) = d\\
2^d &= I_1/I_2\\
\end{aligned}
$$


---

# `power.t.test` - Sample size estimation

What __sample size__ do we need, given 
a significance level of $0.05$ if we want to detect a biologically relevant difference of $50\%$
given a standard deviation of $0.5$ and a power of $0.8$?

.remark-code[
```{r}
res <- power.t.test(delta = 0.59, sd = 0.5, sig.level = 0.05, power = 0.8)
res$n
ceiling(res$n)
```

n - number of observations (per group)


]



---

# `power.t.test` - summary

.footnote[Top - greater standard deviation requires larger sample sizes, Bottom - smaller effect size requires larger sample sizes]

.pull-left[
For each statistical test,<br/> 
there exists a unique relation between:

- desired smallest detectable effect size $\mu$
- sample variance $\sigma^2$
- sample size $N$
- critical p-value $p_0$
- statistical power

]

.right-plot[
```{r echo=FALSE}
sd <- seq(0.5, 2, by = .001)
dd <- power.t.test(delta = 2, sd = 1, sig.level = 0.05, power = 0.8)
powersd <- function(sd, delta = 1, sig.level = 0.05, power = 0.8){
  ceiling(power.t.test(delta = 2, sd = sd, sig.level = sig.level, power = power)$n)
}

ressd <- sapply(sd, powersd)
power_delta <- function(delta, sd = 1, sig.level = 0.05, power = 0.8){
  ceiling(power.t.test(delta = delta, sd = sd, sig.level = sig.level, power = power)$n)
}

delta <- seq(0.5,6, by = 0.001)
resdelta <- sapply(delta, power_delta)
plot(sd, ressd, ylab = "N", type = "l", main = "mu=1, p=0.05, power = 0.8")
plot(delta, resdelta, xlab = "mu", ylab = "N", type = "l", log = "xy", main = "sd=1, p = 0.05, power = 0.8")

```
]

---


# Sample Size - Sample size calculation

.footnote[Top - greater standard deviation requires larger sample sizes, Bottom - smaller effect size requires larger sample sizes]

.pull-left[
For each statistical test,<br/> 
there exists a unique relation between:

- desired smallest detectable effect size $\mu$
- sample variance $\sigma^2$
- sample size $N$
- critical p-value $p_0$
- statistical power

]

.right-plot[
```{r echo=FALSE}
sd <- seq(0.5, 2, by = .001)
dd <- power.t.test(delta = 2, sd = 1, sig.level = 0.05, power = 0.8)
powersd <- function(sd, delta = 1, sig.level = 0.05, power = 0.8){
  ceiling(power.t.test(delta = 2, sd = sd, sig.level = sig.level, power = power)$n)
}

ressd <- sapply(sd, powersd)
power_delta <- function(delta, sd = 1, sig.level = 0.05, power = 0.8){
  ceiling(power.t.test(delta = delta, sd = sd, sig.level = sig.level, power = power)$n)
}

delta <- seq(0.5,6, by = 0.001)
resdelta <- sapply(delta, power_delta)
plot(sd, ressd, ylab = "N", type = "l", main = "mu=1, p=0.05, power = 0.8")
plot(delta, resdelta, xlab = "mu", ylab = "N", type = "l", log = "xy", main = "sd=1, p = 0.05, power = 0.8")

```
]
---


# Bias

Bias is a disproportionate weight in favor of or against an idea or thing, usually in a way that is closed-minded, prejudicial, or unfair.


- Confirmation bias,<br/> tendency to favor information that confirm hypothesis $^{\star}$

.img-right[
![](../inst/images/Man_In_The_Moon_1.png)
]


--

- [catalogofbias.org](https://catalogofbias.org/biases/)

.img-right[
![](../inst/images/Man_In_The_Moon_2.png)
]

--

- ...
  
.img-right[
![](../inst/images/Man_In_The_Moon_3.png)
]

--



.footnote[
$^\star$ Do you examine an experiment which <br/>
fits your hypothesis, for a technical problem? </br>


- [Meta-assessment of bias in science](https://www.pnas.org/content/114/14/3714)</br>
]

---

# Bias


- Publication bias,<br/> Publishing only results that show a significant finding,</br> and inserts bias in favor of positive results.  $^{\star\star}$

.img-right[
![](../inst/images/Z_plot.png)
]



.footnote[
- [Publication bias or research misconduct?](https://medianwatch.netlify.app/post/z_values/)


$^{\star\star}$ A consequence of this is that researchers are unduly motivated<br/>
to manipulate their practices to ensure that<br/> a statistically significant result is reported. <br/>
]


---


# Conclusion

- Understand what heuristics we use to make judgements and what biases might arise.
- The scientific method
- Bradford Hill criteria to study Causality
- Study Types
- Randomization and blinding
- Sample size estimation

__Thank you__

